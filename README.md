<div align="center">

<img src="assets/logo.png" alt="TangoBee Logo" width="120" height="120"/>

# ğŸ TangoBee Local Arena

**The Ultimate Peer-to-Peer AI Model Evaluation Tool**

*Run the same revolutionary evaluation system used by [TangoBee Arena](https://tangobee.sillymode.com) locally with your own OpenRouter API key!*

**ğŸŒ Live Arena:** [tangobee.sillymode.com](https://tangobee.sillymode.com)  
**ğŸ“Š Local Version:** Run evaluations on your machine with any models!

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![OpenRouter](https://img.shields.io/badge/OpenRouter-API-green.svg)](https://openrouter.ai/)
[![TangoBee](https://img.shields.io/badge/TangoBee-Arena-gold.svg)](https://tangobee.sillymode.com)
[![GitHub Stars](https://img.shields.io/github/stars/renmaotian/tangobee-arena?style=social)](https://github.com/renmaotian/tangobee-arena)
[![GitHub Forks](https://img.shields.io/github/forks/renmaotian/tangobee-arena?style=social)](https://github.com/renmaotian/tangobee-arena)

*Where AI models dance together in evaluation harmony* ğŸ•ºğŸ’ƒ

---

</div>

## ğŸŒŸ What is TangoBee Arena?

<div align="center">

### ğŸŒ **Official TangoBee Arena Local Version**

**Live Arena:** [tangobee.sillymode.com](https://tangobee.sillymode.com) | **Local Version:** This Repository

</div>

**Finally, a fair way to evaluate AI models!** ğŸ¯

TangoBee Local Arena brings the revolutionary "tango" evaluation methodology to your local machine. Unlike traditional benchmarks that can be gamed or biased, this tool implements a **true peer-to-peer AI evaluation system** where AI models evaluate each other across five critical domains:

| Domain | ğŸ§  Reasoning | ğŸ’» Coding | ğŸ—£ï¸ Language | ğŸ”¢ Mathematics | ğŸ¨ Creativity |
|--------|--------------|-----------|-------------|----------------|---------------|
| **Focus** | Logic & Critical Thinking | Programming & Development | Communication & Understanding | Problem Solving & Proofs | Innovation & Originality |
| **Example** | Complex reasoning puzzles | Algorithm implementation | Nuanced text analysis | Geometric calculations | Creative problem solving |

### ğŸ­ Why "Tango" Evaluation?

> *"It takes two to tango"* - and it takes multiple AI models to fairly evaluate each other!

âŒ **Traditional benchmarks:** Static questions, human bias, training data contamination  
âœ… **TangoBee method:** Dynamic questions, peer evaluation, eliminated bias

### The "Tango" Methodology

Our revolutionary three-phase evaluation process ensures fair, unbiased assessment:

1. **Phase 1: Question Generation** - Each model generates questions for all domains
2. **Phase 2: Answer Collection** - All models answer all questions from all other models  
3. **Phase 3: Peer Evaluation** - Each model evaluates answers to its own questions only

This eliminates single-point bias and creates a truly peer-to-peer evaluation ecosystem!

## ğŸ“Š Complete TangoBee Arena Leaderboard

*Here are the latest rankings from the live TangoBee Arena (as of June 17, 2025):*

<div align="center">

### ğŸ† **Full Rankings - All 9 Models Evaluated**

</div>

| ğŸ† Rank | Model | Total | ğŸ§  Reasoning | ğŸ’» Coding | ğŸ—£ï¸ Language | ğŸ”¢ Math | ğŸ¨ Creativity |
|---------|--------|-------|-------------|----------|-------------|---------|---------------|
| ğŸ¥‡ **#1** | **OpenAI/O3 Pro** | **8.72** | 8.14 | **9.29** | 8.43 | **9.50** | 8.25 |
| ğŸ¥ˆ **#2** | **Anthropic/Opus 4** | **8.32** | 7.71 | 8.13 | **9.38** | 7.75 | **8.63** |
| ğŸ¥‰ **#3** | **Anthropic/Sonnet 4** | **8.27** | **8.43** | 7.14 | 8.86 | 8.29 | **8.63** |
| **#4** | OpenAI/O3 | **8.09** | 8.17 | 8.86 | 8.00 | 7.29 | 8.13 |
| **#5** | Google/2.5 Pro Preview | **7.78** | **9.00** | 6.88 | 8.50 | 7.00 | 7.50 |
| **#6** | Anthropic/3.7 Sonnet | **7.77** | 7.86 | 6.38 | 8.63 | 7.13 | **8.88** |
| **#7** | OpenAI/O4 Mini | **7.35** | 7.33 | 8.00 | 7.86 | 7.29 | 6.25 |
| **#8** | OpenAI/O4 Mini High | **7.15** | 6.83 | 7.29 | 7.57 | 7.33 | 6.75 |
| **#9** | DeepSeek/R1 0528 | **6.96** | 7.71 | 4.50 | 7.57 | 7.14 | 7.88 |

<div align="center">

*ğŸ”¥ **Run the exact same evaluation system that generated these results!** ğŸ”¥*

**See these results live at:** [tangobee.sillymode.com](https://tangobee.sillymode.com)

</div>

**ğŸ¯ Complete Performance Analysis:**

### ğŸ… **Domain Champions**
- **ğŸ§  Reasoning Leader:** Google/2.5 Pro Preview (9.00)
- **ğŸ’» Coding Champion:** OpenAI/O3 Pro (9.29) 
- **ğŸ—£ï¸ Language Master:** Anthropic/Opus 4 (9.38)
- **ğŸ”¢ Math Wizard:** OpenAI/O3 Pro (9.50)
- **ğŸ¨ Creativity King:** Anthropic/3.7 Sonnet (8.88)

### ğŸ“Š **Key Insights**
- ğŸ¯ **Tight Competition:** Top 3 models within 0.45 points
- ğŸš€ **OpenAI O3 Pro** dominates quantitative domains (coding + math)
- ğŸ—£ï¸ **Anthropic models** excel in creative and language tasks
- ğŸ“ˆ **Google 2.5 Pro** leads reasoning despite overall #5 ranking
- ğŸ’ª **Each model has unique strengths** - no single "perfect" model
- ğŸ”„ **Fair evaluation** - every model evaluated by every other model

*Ready to discover which models perform best in YOUR evaluation? ğŸ*

## ğŸš€ Quick Start

### ğŸ¯ **Get Started in Under 5 Minutes!**

<div align="center">

**From zero to running evaluations in 3 simple steps** âš¡

</div>

### Prerequisites

- **Python 3.8+** ([Download here](https://www.python.org/downloads/))
- **OpenRouter API key** ([Get yours FREE here](https://openrouter.ai/keys)) 
- **5 minutes of your time** â°

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/renmaotian/tangobee-arena.git
   cd tangobee-arena
   ```

2. **Run the setup:**
   ```bash
   python setup.py
   ```

3. **Configure your API key:**
   
   **Option A: Environment variable (recommended)**
   ```bash
   export OPENROUTER_API_KEY=your_openrouter_api_key_here
   ```
   
   **Option B: Config file**
   ```bash
   # Edit config/api_key.txt.template and rename to api_key.txt
   cp config/api_key.txt.template config/api_key.txt
   # Edit the file and add your API key
   ```

4. **Run your first evaluation:**
   ```bash
   python run_evaluation.py --quick
   ```

<div align="center">

**ğŸ‰ That's it! Your beautiful HTML report will be ready in `reports/`! ğŸ‰**

*Sample run: 2 models evaluated in 3 minutes for ~$0.75*

</div>

### ğŸƒâ€â™‚ï¸ **Even Faster Start**

Want to dive in immediately? Use this one-liner:

```bash
git clone https://github.com/renmaotian/tangobee-arena.git && cd tangobee-arena && python setup.py && export OPENROUTER_API_KEY=your_key_here && python run_evaluation.py --quick
```

Replace `your_key_here` with your OpenRouter API key and you're ready to dance! ğŸ•º

### ğŸŒŸ **What Users Are Saying**

> *"Finally, an AI evaluation system that doesn't feel rigged! The peer-to-peer approach is genius."*  
> â€” AI Researcher

> *"Love seeing which models excel in different domains. O3 Pro crushing math while Opus 4 dominates language!"*  
> â€” ML Engineer

> *"Setup was literally 2 minutes. The HTML reports are beautiful and the trend analysis is incredibly useful."*  
> â€” Data Scientist

<div align="center">

**Join hundreds of developers already running TangoBee evaluations!** ğŸš€

</div>

## ğŸ“Š Features

### âœ¨ Core Capabilities

- **ğŸ”„ Peer-to-Peer Evaluation** - True AI-evaluating-AI methodology
- **ğŸ“ˆ Trend Analysis** - Track performance over time with beautiful charts
- **ğŸ¯ Same-Day Overwriting** - Run multiple evaluations per day
- **ğŸ“± Responsive HTML Reports** - Beautiful, mobile-friendly results
- **ğŸŒ Multi-Model Support** - Works with any OpenRouter-supported model
- **ğŸ’° Cost-Effective** - Uses cheap models by default (GPT-4o-mini, Claude-3.5-haiku)

### ğŸ“Š Generated Visualizations

- **Performance Heatmaps** - See how each model rates others
- **Domain-Specific Analysis** - Detailed breakdown by capability area
- **Trend Plots** - 6-month historical performance tracking
- **Comprehensive Reports** - All data in beautiful HTML format

## ğŸ› ï¸ Usage

### Basic Usage

```bash
# Quick evaluation with 2 cheap models
python run_evaluation.py --quick

# Full evaluation with default models
python run_evaluation.py

# Custom models
python run_evaluation.py --models openai/gpt-4o anthropic/claude-3-5-sonnet

# Direct API usage
python src/local_arena.py --api-key your_key_here
```

### Configuration

**ğŸ”¥ Cutting-Edge Models Supported** (in `config/models.txt`):

| Model | Provider | Strengths | Cost Level |
|-------|----------|-----------|------------|
| `openai/o3-pro` | OpenAI | ğŸ§  Advanced reasoning | Premium |
| `openai/o3` | OpenAI | ğŸ¯ Latest capabilities | High |
| `anthropic/claude-opus-4` | Anthropic | ğŸ¨ Creative excellence | Premium |
| `anthropic/claude-sonnet-4` | Anthropic | âš¡ Fast & capable | Medium |
| `google/gemini-2.5-pro-preview` | Google | ğŸŒŸ Multimodal power | High |
| `deepseek/deepseek-r1-0528` | DeepSeek | ğŸ’» Coding mastery | Budget |
| `openai/gpt-4o-mini` | OpenAI | ğŸ’° Cost-effective | Budget |
| `anthropic/claude-3.5-haiku` | Anthropic | ğŸš€ Fast reasoning | Budget |

*Mix and match any models for your perfect evaluation setup!*

**Estimated Costs:**
- 2 models (quick): ~$0.50-1.00
- 3 models (default): ~$2.00-4.00  
- 5 models: ~$10.00-20.00

*Costs depend on model pricing and response lengths*

## ğŸ“ Project Structure

```
tangobee-local-arena/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ local_arena.py          # Main evaluation engine
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ models.txt              # Default models configuration
â”‚   â””â”€â”€ api_key.txt.template    # API key template
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ logo.png               # TangoBee logo
â”œâ”€â”€ data/                      # Local database and logs
â”œâ”€â”€ reports/                   # Generated HTML reports
â”‚   â””â”€â”€ visualizations/        # Charts and heatmaps
â”œâ”€â”€ setup.py                   # Easy setup script
â”œâ”€â”€ run_evaluation.py          # User-friendly runner
â””â”€â”€ requirements.txt           # Python dependencies
```

## ğŸ”§ Advanced Usage

### Adding Custom Models

Edit `config/models.txt`:
```
openai/gpt-4o-mini
anthropic/claude-3-5-haiku
google/gemini-flash-1.5
meta/llama-3.1-8b-instruct
```

### API Key Management

**Environment Variable:**
```bash
export OPENROUTER_API_KEY=sk-or-v1-xxxxx
```

**Config File:**
```bash
echo "sk-or-v1-xxxxx" > config/api_key.txt
```

### Custom Evaluation

```python
from src.local_arena import TangoBeeLocalArena

arena = TangoBeeLocalArena(api_key="your-key")
report_path = arena.run_full_evaluation([
    "openai/gpt-4o-mini",
    "anthropic/claude-3-5-haiku"
])
print(f"Report generated: {report_path}")
```

## ğŸ“ˆ Understanding Results

### Scoring System
- **Scale:** 0-10 (higher is better)
- **Methodology:** Each model scores answers to its own questions
- **Aggregation:** Domain scores averaged for total score

### Report Sections
1. **ğŸ† Leaderboard** - Final rankings with medals
2. **ğŸ“Š Performance Heatmaps** - Model-vs-model evaluation matrix
3. **ğŸ“ˆ Trend Analysis** - Historical performance tracking

### Interpreting Heatmaps
- **Rows:** Evaluating models (judges)
- **Columns:** Evaluated models (participants)
- **Colors:** Green = high scores, Red = low scores
- **Numbers:** Average scores given by each judge

## ğŸ” Troubleshooting

### Common Issues

**Error: API key not found**
```bash
# Set environment variable
export OPENROUTER_API_KEY=your_key

# Or create config file
echo "your_key" > config/api_key.txt
```

**Error: Rate limiting**
- The tool includes automatic retry logic
- Consider using fewer models for testing
- Check your OpenRouter account limits

**Error: Import errors**
```bash
# Reinstall dependencies
pip install -r requirements.txt
```

**Slow performance**
- Use `--quick` flag for faster testing
- Choose faster models (gpt-4o-mini, claude-3.5-haiku)
- Reduce number of models being evaluated

### Debug Mode

```bash
# Check logs
tail -f data/tangobee_local.log

# Verbose output
python src/local_arena.py --api-key your_key --models openai/gpt-4o-mini
```

## ğŸ† Why Choose TangoBee Local Arena?

### ğŸ¯ **Unbiased & Fair**
- âœ… No single entity controls evaluation criteria
- âœ… Models evaluate each other, eliminating human bias
- âœ… Dynamic questions prevent training data contamination
- âœ… Peer-to-peer methodology ensures fairness

### ğŸš€ **Easy & Accessible**
- âœ… One-command setup with `python setup.py`
- âœ… Works with any OpenRouter-supported model
- âœ… Beautiful HTML reports with visualizations
- âœ… Budget-friendly default model selection

### ğŸ“Š **Comprehensive & Insightful**
- âœ… Five distinct evaluation domains
- âœ… Historical trend analysis
- âœ… Performance heatmaps
- âœ… Detailed statistical breakdowns

### ğŸŒŸ **Proven Methodology**
- âœ… **Official local version** of [tangobee.sillymode.com](https://tangobee.sillymode.com)
- âœ… **Battle-tested** with weekly live evaluations
- âœ… **Continuously refined** system based on real usage
- âœ… **Community-validated** evaluation approach

## ğŸ¤ Contributing

We welcome contributions! This is the official local version of [TangoBee Arena](https://tangobee.sillymode.com).

### Development Setup

```bash
git clone https://github.com/yourusername/tangobee-local-arena.git
cd tangobee-local-arena
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Areas for Contribution

- ğŸ“Š Additional visualization types
- ğŸ”§ New evaluation domains
- ğŸ¯ Performance optimizations
- ğŸ“± Mobile app version
- ğŸŒ Web interface
- ğŸ“ˆ Advanced analytics

## ğŸ“ License

This project is open source and available under the [MIT License](LICENSE).

## ğŸ™ Acknowledgments

- **TangoBee Arena** - Original inspiration and methodology
- **OpenRouter** - API platform enabling model access
- **The AI Community** - For advancing model evaluation techniques

## ğŸ“ Support

- ğŸ› **Issues:** [GitHub Issues](https://github.com/renmaotian/tangobee-arena/issues)
- ğŸ’¬ **Discussions:** [GitHub Discussions](https://github.com/renmaotian/tangobee-arena/discussions)
- ğŸŒ **Live Arena:** [tangobee.sillymode.com](https://tangobee.sillymode.com)

---

<div align="center">

**ğŸ Made with the TangoBee evaluation methodology ğŸ**

*Where AI models dance together in evaluation harmony*

</div>